{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import warnings\n",
    "import os.path\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# For simple regression problem\n",
    "TRAINING_POINTS = 1000\n",
    "\n",
    "# For fashion-MNIST and similar problems\n",
    "DATA_ROOT = '/data/cs3450/data/'\n",
    "FASHION_MNIST_TRAINING = '/data/cs3450/data/fashion_mnist_flattened_training.npz'\n",
    "FASHION_MNIST_TESTING = '/data/cs3450/data/fashion_mnist_flattened_testing.npz'\n",
    "CIFAR10_TRAINING = '/data/cs3450/data/cifar10_flattened_training.npz'\n",
    "CIFAR10_TESTING = '/data/cs3450/data/cifar10_flattened_testing.npz'\n",
    "CIFAR100_TRAINING = '/data/cs3450/data/cifar100_flattened_training.npz'\n",
    "CIFAR100_TESTING = '/data/cs3450/data/cifar100_flattened_testing.npz'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the GPU Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\n",
    "       https://d2l.ai/chapter_deep-learning-computation/use-gpu.html\n",
    "    \"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "DEVICE=try_gpu()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_training_data():\n",
    "    \"\"\"\n",
    "    This method simply rotates points in a 2D space.\n",
    "    Be sure to use L2 regression in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_folded_training_data():\n",
    "    \"\"\"\n",
    "    This method introduces a single non-linear fold into the sort of data created by create_linear_training_data. Be sure to REMOVE the final softmax layer before testing on this data!\n",
    "    Be sure to use L2 regression in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    x2 *= 2 * ((x2 > 0).float() - 0.5)\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_square():\n",
    "    \"\"\"\n",
    "    This is the square example that we looked at in class.\n",
    "    insideness is true if the points are inside the square.\n",
    "    :return: (points, insideness) the dataset. points is a 2xN array of points and insideness is true if the point is inside the square.\n",
    "    \"\"\"\n",
    "    win_x = [2,2,3,3]\n",
    "    win_y = [1,2,2,1]\n",
    "    win = torch.tensor([win_x,win_y],dtype=torch.float32)\n",
    "    win_rot = torch.cat((win[:,1:],win[:,0:1]),axis=1)\n",
    "    t = win_rot - win # edges tangent along side of poly\n",
    "    rotation = torch.tensor([[0, 1],[-1,0]],dtype=torch.float32)\n",
    "    normal = rotation @ t # normal vectors to each side of poly\n",
    "        # torch.matmul(rotation,t) # Same thing\n",
    "\n",
    "    points = torch.rand((2,2000),dtype = torch.float32)\n",
    "    points = 4*points\n",
    "\n",
    "    vectors = points[:,np.newaxis,:] - win[:,:,np.newaxis] # reshape to fill origin\n",
    "    insideness = (normal[:,:,np.newaxis] * vectors).sum(axis=0)\n",
    "    insideness = insideness.T\n",
    "    insideness = insideness > 0\n",
    "    insideness = insideness.all(axis=1)\n",
    "    return points, insideness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_flattened(train=True,dataset='Fashion-MNIST',download=False):\n",
    "    \"\"\"\n",
    "    :param train: True for training, False for testing\n",
    "    :param dataset: 'Fashion-MNIST', 'CIFAR-10', or 'CIFAR-100'\n",
    "    :param download: True to download. Keep to false afterwords to avoid unneeded downloads.\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    if dataset == 'Fashion-MNIST':\n",
    "        if train:\n",
    "            path = FASHION_MNIST_TRAINING\n",
    "        else:\n",
    "            path = FASHION_MNIST_TESTING\n",
    "        num_labels = 10\n",
    "    elif dataset == 'CIFAR-10':\n",
    "        if train:\n",
    "            path = CIFAR10_TRAINING\n",
    "        else:\n",
    "            path = CIFAR10_TESTING\n",
    "        num_labels = 10\n",
    "    elif dataset == 'CIFAR-100':\n",
    "        if train:\n",
    "            path = CIFAR100_TRAINING\n",
    "        else:\n",
    "            path = CIFAR100_TESTING\n",
    "        num_labels = 100\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset: '+str(dataset))\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        print('Loading cached flattened data for',dataset,'training' if train else 'testing')\n",
    "        data = np.load(path)\n",
    "        x = torch.tensor(data['x'],dtype=torch.float32)\n",
    "        y = torch.tensor(data['y'],dtype=torch.float32)\n",
    "        pass\n",
    "    else:\n",
    "        class ToTorch(object):\n",
    "            \"\"\"Like ToTensor, only to a numpy array\"\"\"\n",
    "\n",
    "            def __call__(self, pic):\n",
    "                return torchvision.transforms.functional.to_tensor(pic)\n",
    "\n",
    "        if dataset == 'Fashion-MNIST':\n",
    "            data = torchvision.datasets.FashionMNIST(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        elif dataset == 'CIFAR-10':\n",
    "            data = torchvision.datasets.CIFAR10(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        elif dataset == 'CIFAR-100':\n",
    "            data = torchvision.datasets.CIFAR100(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        else:\n",
    "            raise ValueError('This code should be unreachable because of a previous check.')\n",
    "        x = torch.zeros((len(data[0][0].flatten()), len(data)),dtype=torch.float32)\n",
    "        for index, image in enumerate(data):\n",
    "            x[:, index] = data[index][0].flatten()\n",
    "        labels = torch.tensor([sample[1] for sample in data])\n",
    "        y = torch.zeros((num_labels, len(labels)), dtype=torch.float32)\n",
    "        y[labels, torch.arange(len(labels))] = 1\n",
    "        np.savez(path, x=x.detach().numpy(), y=y.detach().numpy())\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached flattened data for Fashion-MNIST training\n"
     ]
    }
   ],
   "source": [
    "# TODO: Select your datasource.\n",
    "dataset = 'Fashion-MNIST'\n",
    "#ataset = 'CIFAR-10'\n",
    "# dataset = 'CIFAR-100'\n",
    "\n",
    "#x_train, y_train = create_linear_training_data()\n",
    "#x_train, y_train = create_folded_training_data()\n",
    "#points_train, insideness_train = create_square()\n",
    "x_train, y_train = load_dataset_flattened(train=True, dataset=dataset, download=False)\n",
    "# Move selected datasets to GPU\n",
    "x_train = x_train.to(DEVICE)\n",
    "y_train = y_train.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached flattened data for Fashion-MNIST testing\n"
     ]
    }
   ],
   "source": [
    "# TODO: Test the accuracy of your network\n",
    "#x_test, y_test = create_linear_training_data()\n",
    "x_test, y_test = load_dataset_flattened(train=False, dataset=dataset, download=False)\n",
    "\n",
    "# Move the selected datasets to the GPU\n",
    "x_test = x_test.to(DEVICE)\n",
    "y_test = y_test.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4 - Tuning your Autograd Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batceh_size, weight_decay):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param *= weight_decay\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing/Experiment Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 60000])\n",
      "torch.Size([10, 60000])\n",
      "torch.Size([784, 10000])\n",
      "torch.Size([784, 10000])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X,w,b):\n",
    "    #print(\"W:\", w)\n",
    "    #print(\"B:\", b)\n",
    "    return ((torch.matmul(X,w) + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of MiniBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    n = len(features)\n",
    "    iss = list(range(n))\n",
    "    random.shuffle(iss)\n",
    "    for i in range(0, n, batch_size):\n",
    "        b = torch.tensor(iss[i:min(i + batch_size, n)])\n",
    "        yield features[b], labels[b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stablesoftmax(x):\n",
    "    x = x.float()\n",
    "    f = x - torch.max(x)\n",
    "    log = f - torch.log(torch.sum(torch.exp(f)))\n",
    "    return log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x.float()\n",
    "    m = torch.max(x.float(),0)\n",
    "    v = torch.exp(x.float() - m.values)\n",
    "    b = torch.sum(v,0).float()\n",
    "    t = (v / b).float()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(X,y):\n",
    "    #epsilon = 1e-5\n",
    "    y_hat = stablesoftmax(X)\n",
    "    prob = y * (y_hat)\n",
    "    return -torch.sum(prob) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    y = torch.argmax(y, dim=0)\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum()) / 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop For Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \n",
      "Loss: 124.0457992553711 \n",
      "Accuracy:  0.0592\n",
      "Epoch:  2 \n",
      "Loss: 77.9225845336914 \n",
      "Accuracy:  0.0577\n",
      "Epoch:  3 \n",
      "Loss: 32.828857421875 \n",
      "Accuracy:  0.1338\n",
      "Epoch:  4 \n",
      "Loss: 11.437917709350586 \n",
      "Accuracy:  0.3204\n",
      "Epoch:  5 \n",
      "Loss: 11.424445152282715 \n",
      "Accuracy:  0.3387\n",
      "Epoch:  6 \n",
      "Loss: 11.414013862609863 \n",
      "Accuracy:  0.3635\n",
      "Epoch:  7 \n",
      "Loss: 11.404820442199707 \n",
      "Accuracy:  0.3837\n",
      "Epoch:  8 \n",
      "Loss: 11.396069526672363 \n",
      "Accuracy:  0.3876\n",
      "Epoch:  9 \n",
      "Loss: 11.385401725769043 \n",
      "Accuracy:  0.3977\n",
      "Epoch:  10 \n",
      "Loss: 11.375394821166992 \n",
      "Accuracy:  0.428\n",
      "Epoch:  11 \n",
      "Loss: 11.369349479675293 \n",
      "Accuracy:  0.4472\n",
      "Epoch:  12 \n",
      "Loss: 11.357158660888672 \n",
      "Accuracy:  0.4563\n",
      "Epoch:  13 \n",
      "Loss: 11.348383903503418 \n",
      "Accuracy:  0.479\n",
      "Epoch:  14 \n",
      "Loss: 11.339290618896484 \n",
      "Accuracy:  0.4948\n",
      "Epoch:  15 \n",
      "Loss: 11.331463813781738 \n",
      "Accuracy:  0.5081\n",
      "Epoch:  16 \n",
      "Loss: 11.320676803588867 \n",
      "Accuracy:  0.5405\n",
      "Epoch:  17 \n",
      "Loss: 11.311068534851074 \n",
      "Accuracy:  0.5551\n",
      "Epoch:  18 \n",
      "Loss: 11.301878929138184 \n",
      "Accuracy:  0.5561\n",
      "Epoch:  19 \n",
      "Loss: 11.292534828186035 \n",
      "Accuracy:  0.5718\n",
      "Epoch:  20 \n",
      "Loss: 11.28355598449707 \n",
      "Accuracy:  0.5895\n",
      "Epoch:  21 \n",
      "Loss: 11.275294303894043 \n",
      "Accuracy:  0.5841\n",
      "Epoch:  22 \n",
      "Loss: 11.266477584838867 \n",
      "Accuracy:  0.5943\n",
      "Epoch:  23 \n",
      "Loss: 11.25841236114502 \n",
      "Accuracy:  0.5907\n",
      "Epoch:  24 \n",
      "Loss: 11.24666690826416 \n",
      "Accuracy:  0.5981\n",
      "Epoch:  25 \n",
      "Loss: 11.238507270812988 \n",
      "Accuracy:  0.6172\n",
      "Epoch:  26 \n",
      "Loss: 11.229079246520996 \n",
      "Accuracy:  0.6125\n",
      "Epoch:  27 \n",
      "Loss: 11.220540046691895 \n",
      "Accuracy:  0.6037\n",
      "Epoch:  28 \n",
      "Loss: 11.21034049987793 \n",
      "Accuracy:  0.6197\n",
      "Epoch:  29 \n",
      "Loss: 11.201574325561523 \n",
      "Accuracy:  0.6138\n",
      "Epoch:  30 \n",
      "Loss: 11.192451477050781 \n",
      "Accuracy:  0.6182\n",
      "Epoch:  31 \n",
      "Loss: 11.183863639831543 \n",
      "Accuracy:  0.6289\n",
      "Epoch:  32 \n",
      "Loss: 11.175164222717285 \n",
      "Accuracy:  0.6222\n",
      "Epoch:  33 \n",
      "Loss: 11.166828155517578 \n",
      "Accuracy:  0.613\n",
      "Epoch:  34 \n",
      "Loss: 11.157845497131348 \n",
      "Accuracy:  0.6271\n",
      "Epoch:  35 \n",
      "Loss: 11.148895263671875 \n",
      "Accuracy:  0.6213\n",
      "Epoch:  36 \n",
      "Loss: 11.139839172363281 \n",
      "Accuracy:  0.625\n",
      "Epoch:  37 \n",
      "Loss: 11.130913734436035 \n",
      "Accuracy:  0.6285\n",
      "Epoch:  38 \n",
      "Loss: 11.121601104736328 \n",
      "Accuracy:  0.6362\n",
      "Epoch:  39 \n",
      "Loss: 11.113739967346191 \n",
      "Accuracy:  0.6339\n",
      "Epoch:  40 \n",
      "Loss: 11.104336738586426 \n",
      "Accuracy:  0.6379\n",
      "Epoch:  41 \n",
      "Loss: 11.096444129943848 \n",
      "Accuracy:  0.6416\n",
      "Epoch:  42 \n",
      "Loss: 11.089043617248535 \n",
      "Accuracy:  0.6445\n",
      "Epoch:  43 \n",
      "Loss: 11.078615188598633 \n",
      "Accuracy:  0.6308\n",
      "Epoch:  44 \n",
      "Loss: 11.070662498474121 \n",
      "Accuracy:  0.6378\n",
      "Epoch:  45 \n",
      "Loss: 11.062324523925781 \n",
      "Accuracy:  0.6377\n",
      "Epoch:  46 \n",
      "Loss: 11.052165031433105 \n",
      "Accuracy:  0.6407\n",
      "Epoch:  47 \n",
      "Loss: 11.044843673706055 \n",
      "Accuracy:  0.6444\n",
      "Epoch:  48 \n",
      "Loss: 11.035268783569336 \n",
      "Accuracy:  0.6483\n",
      "Epoch:  49 \n",
      "Loss: 11.028044700622559 \n",
      "Accuracy:  0.6459\n",
      "Epoch:  50 \n",
      "Loss: 11.019240379333496 \n",
      "Accuracy:  0.6438\n",
      "Epoch:  51 \n",
      "Loss: 11.009910583496094 \n",
      "Accuracy:  0.6425\n",
      "Epoch:  52 \n",
      "Loss: 11.002579689025879 \n",
      "Accuracy:  0.6433\n",
      "Epoch:  53 \n",
      "Loss: 10.993115425109863 \n",
      "Accuracy:  0.6501\n",
      "Epoch:  54 \n",
      "Loss: 10.98590087890625 \n",
      "Accuracy:  0.6438\n",
      "Epoch:  55 \n",
      "Loss: 10.976679801940918 \n",
      "Accuracy:  0.6495\n",
      "Epoch:  56 \n",
      "Loss: 10.968574523925781 \n",
      "Accuracy:  0.6498\n",
      "Epoch:  57 \n",
      "Loss: 10.962071418762207 \n",
      "Accuracy:  0.6459\n",
      "Epoch:  58 \n",
      "Loss: 10.953676223754883 \n",
      "Accuracy:  0.6413\n",
      "Epoch:  59 \n",
      "Loss: 10.944433212280273 \n",
      "Accuracy:  0.6508\n",
      "Epoch:  60 \n",
      "Loss: 10.937010765075684 \n",
      "Accuracy:  0.6425\n",
      "Epoch:  61 \n",
      "Loss: 10.928812980651855 \n",
      "Accuracy:  0.651\n",
      "Epoch:  62 \n",
      "Loss: 10.924208641052246 \n",
      "Accuracy:  0.6472\n",
      "Epoch:  63 \n",
      "Loss: 10.91457748413086 \n",
      "Accuracy:  0.6465\n",
      "Epoch:  64 \n",
      "Loss: 10.907001495361328 \n",
      "Accuracy:  0.6534\n",
      "Epoch:  65 \n",
      "Loss: 10.898445129394531 \n",
      "Accuracy:  0.6526\n",
      "Epoch:  66 \n",
      "Loss: 10.891641616821289 \n",
      "Accuracy:  0.6527\n",
      "Epoch:  67 \n",
      "Loss: 10.883262634277344 \n",
      "Accuracy:  0.6585\n",
      "Epoch:  68 \n",
      "Loss: 10.876089096069336 \n",
      "Accuracy:  0.6551\n",
      "Epoch:  69 \n",
      "Loss: 10.868623733520508 \n",
      "Accuracy:  0.6602\n",
      "Epoch:  70 \n",
      "Loss: 10.862638473510742 \n",
      "Accuracy:  0.6569\n",
      "Epoch:  71 \n",
      "Loss: 10.85507583618164 \n",
      "Accuracy:  0.6493\n",
      "Epoch:  72 \n",
      "Loss: 10.847636222839355 \n",
      "Accuracy:  0.6566\n",
      "Epoch:  73 \n",
      "Loss: 10.8407621383667 \n",
      "Accuracy:  0.659\n",
      "Epoch:  74 \n",
      "Loss: 10.834151268005371 \n",
      "Accuracy:  0.6603\n",
      "Epoch:  75 \n",
      "Loss: 10.826743125915527 \n",
      "Accuracy:  0.6612\n",
      "Epoch:  76 \n",
      "Loss: 10.820351600646973 \n",
      "Accuracy:  0.6658\n",
      "Epoch:  77 \n",
      "Loss: 10.813169479370117 \n",
      "Accuracy:  0.6581\n",
      "Epoch:  78 \n",
      "Loss: 10.806220054626465 \n",
      "Accuracy:  0.665\n",
      "Epoch:  79 \n",
      "Loss: 10.80075740814209 \n",
      "Accuracy:  0.6638\n",
      "Epoch:  80 \n",
      "Loss: 10.793174743652344 \n",
      "Accuracy:  0.6678\n",
      "Epoch:  81 \n",
      "Loss: 10.786885261535645 \n",
      "Accuracy:  0.6623\n",
      "Epoch:  82 \n",
      "Loss: 10.781323432922363 \n",
      "Accuracy:  0.6705\n",
      "Epoch:  83 \n",
      "Loss: 10.774974822998047 \n",
      "Accuracy:  0.6594\n",
      "Epoch:  84 \n",
      "Loss: 10.768330574035645 \n",
      "Accuracy:  0.6746\n",
      "Epoch:  85 \n",
      "Loss: 10.76319694519043 \n",
      "Accuracy:  0.668\n",
      "Epoch:  86 \n",
      "Loss: 10.756576538085938 \n",
      "Accuracy:  0.6767\n",
      "Epoch:  87 \n",
      "Loss: 10.751446723937988 \n",
      "Accuracy:  0.6629\n",
      "Epoch:  88 \n",
      "Loss: 10.745196342468262 \n",
      "Accuracy:  0.6558\n",
      "Epoch:  89 \n",
      "Loss: 10.738984107971191 \n",
      "Accuracy:  0.6716\n",
      "Epoch:  90 \n",
      "Loss: 10.733293533325195 \n",
      "Accuracy:  0.666\n",
      "Epoch:  91 \n",
      "Loss: 10.727574348449707 \n",
      "Accuracy:  0.6733\n",
      "Epoch:  92 \n",
      "Loss: 10.723007202148438 \n",
      "Accuracy:  0.6709\n",
      "Epoch:  93 \n",
      "Loss: 10.716383934020996 \n",
      "Accuracy:  0.6647\n",
      "Epoch:  94 \n",
      "Loss: 10.711767196655273 \n",
      "Accuracy:  0.6704\n",
      "Epoch:  95 \n",
      "Loss: 10.706676483154297 \n",
      "Accuracy:  0.6698\n",
      "Epoch:  96 \n",
      "Loss: 10.70058536529541 \n",
      "Accuracy:  0.669\n",
      "Epoch:  97 \n",
      "Loss: 10.695525169372559 \n",
      "Accuracy:  0.6749\n",
      "Epoch:  98 \n",
      "Loss: 10.690988540649414 \n",
      "Accuracy:  0.675\n",
      "Epoch:  99 \n",
      "Loss: 10.686039924621582 \n",
      "Accuracy:  0.675\n",
      "Epoch:  100 \n",
      "Loss: 10.680216789245605 \n",
      "Accuracy:  0.6795\n",
      "Epoch:  101 \n",
      "Loss: 10.67676830291748 \n",
      "Accuracy:  0.6815\n",
      "Epoch:  102 \n",
      "Loss: 10.671114921569824 \n",
      "Accuracy:  0.6815\n",
      "Epoch:  103 \n",
      "Loss: 10.665873527526855 \n",
      "Accuracy:  0.6765\n",
      "Epoch:  104 \n",
      "Loss: 10.661900520324707 \n",
      "Accuracy:  0.6743\n",
      "Epoch:  105 \n",
      "Loss: 10.656441688537598 \n",
      "Accuracy:  0.6841\n",
      "Epoch:  106 \n",
      "Loss: 10.652371406555176 \n",
      "Accuracy:  0.6865\n",
      "Epoch:  107 \n",
      "Loss: 10.648286819458008 \n",
      "Accuracy:  0.6833\n",
      "Epoch:  108 \n",
      "Loss: 10.643115043640137 \n",
      "Accuracy:  0.6852\n",
      "Epoch:  109 \n",
      "Loss: 10.638741493225098 \n",
      "Accuracy:  0.6862\n",
      "Epoch:  110 \n",
      "Loss: 10.634366989135742 \n",
      "Accuracy:  0.6779\n",
      "Epoch:  111 \n",
      "Loss: 10.629932403564453 \n",
      "Accuracy:  0.6894\n",
      "Epoch:  112 \n",
      "Loss: 10.625673294067383 \n",
      "Accuracy:  0.6904\n",
      "Epoch:  113 \n",
      "Loss: 10.621872901916504 \n",
      "Accuracy:  0.6866\n",
      "Epoch:  114 \n",
      "Loss: 10.618449211120605 \n",
      "Accuracy:  0.6848\n",
      "Epoch:  115 \n",
      "Loss: 10.614212036132812 \n",
      "Accuracy:  0.6834\n",
      "Epoch:  116 \n",
      "Loss: 10.609755516052246 \n",
      "Accuracy:  0.6819\n",
      "Epoch:  117 \n",
      "Loss: 10.606674194335938 \n",
      "Accuracy:  0.6853\n",
      "Epoch:  118 \n",
      "Loss: 10.601883888244629 \n",
      "Accuracy:  0.6913\n",
      "Epoch:  119 \n",
      "Loss: 10.597723007202148 \n",
      "Accuracy:  0.6845\n",
      "Epoch:  120 \n",
      "Loss: 10.593936920166016 \n",
      "Accuracy:  0.6839\n",
      "Epoch:  121 \n",
      "Loss: 10.5925931930542 \n",
      "Accuracy:  0.6855\n",
      "Epoch:  122 \n",
      "Loss: 10.586901664733887 \n",
      "Accuracy:  0.6886\n",
      "Epoch:  123 \n",
      "Loss: 10.583884239196777 \n",
      "Accuracy:  0.6837\n",
      "Epoch:  124 \n",
      "Loss: 10.579490661621094 \n",
      "Accuracy:  0.6885\n",
      "Epoch:  125 \n",
      "Loss: 10.57544231414795 \n",
      "Accuracy:  0.6932\n",
      "Epoch:  126 \n",
      "Loss: 10.573287010192871 \n",
      "Accuracy:  0.686\n",
      "Epoch:  127 \n",
      "Loss: 10.56873893737793 \n",
      "Accuracy:  0.6897\n",
      "Epoch:  128 \n",
      "Loss: 10.5656156539917 \n",
      "Accuracy:  0.6977\n",
      "Epoch:  129 \n",
      "Loss: 10.562551498413086 \n",
      "Accuracy:  0.6858\n",
      "Epoch:  130 \n",
      "Loss: 10.558758735656738 \n",
      "Accuracy:  0.6931\n",
      "Epoch:  131 \n",
      "Loss: 10.555146217346191 \n",
      "Accuracy:  0.6889\n",
      "Epoch:  132 \n",
      "Loss: 10.552201271057129 \n",
      "Accuracy:  0.6906\n",
      "Epoch:  133 \n",
      "Loss: 10.550309181213379 \n",
      "Accuracy:  0.6935\n",
      "Epoch:  134 \n",
      "Loss: 10.545899391174316 \n",
      "Accuracy:  0.6941\n",
      "Epoch:  135 \n",
      "Loss: 10.542572021484375 \n",
      "Accuracy:  0.6904\n",
      "Epoch:  136 \n",
      "Loss: 10.540325164794922 \n",
      "Accuracy:  0.6956\n",
      "Epoch:  137 \n",
      "Loss: 10.537007331848145 \n",
      "Accuracy:  0.6972\n",
      "Epoch:  138 \n",
      "Loss: 10.533601760864258 \n",
      "Accuracy:  0.6915\n",
      "Epoch:  139 \n",
      "Loss: 10.53093433380127 \n",
      "Accuracy:  0.6969\n",
      "Epoch:  140 \n",
      "Loss: 10.527732849121094 \n",
      "Accuracy:  0.696\n",
      "Epoch:  141 \n",
      "Loss: 10.524858474731445 \n",
      "Accuracy:  0.696\n",
      "Epoch:  142 \n",
      "Loss: 10.521597862243652 \n",
      "Accuracy:  0.6983\n",
      "Epoch:  143 \n",
      "Loss: 10.518709182739258 \n",
      "Accuracy:  0.6947\n",
      "Epoch:  144 \n",
      "Loss: 10.516195297241211 \n",
      "Accuracy:  0.6985\n",
      "Epoch:  145 \n",
      "Loss: 10.512879371643066 \n",
      "Accuracy:  0.6949\n",
      "Epoch:  146 \n",
      "Loss: 10.511274337768555 \n",
      "Accuracy:  0.6913\n",
      "Epoch:  147 \n",
      "Loss: 10.508465766906738 \n",
      "Accuracy:  0.6907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  148 \n",
      "Loss: 10.50490951538086 \n",
      "Accuracy:  0.6956\n",
      "Epoch:  149 \n",
      "Loss: 10.502558708190918 \n",
      "Accuracy:  0.6907\n",
      "Epoch:  150 \n",
      "Loss: 10.500123977661133 \n",
      "Accuracy:  0.695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Epoch vs Accuracy')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "#for the first layer\n",
    "hidden_layers = 200\n",
    "flw = np.random.rand(784,hidden_layers) * 0.1\n",
    "tflw = torch.tensor(flw, requires_grad=True, device=DEVICE)\n",
    "#print(\"First Layer Weights: \", tflw)\n",
    "flb = torch.zeros(hidden_layers, requires_grad=True, device=DEVICE)\n",
    "#print(flb)\n",
    "\n",
    "#for the second layer\n",
    "slw = np.random.rand(hidden_layers,10) * 0.1\n",
    "tslw = torch.tensor(slw, requires_grad=True, device=DEVICE)\n",
    "#print(tslw)\n",
    "slb = torch.zeros(10, requires_grad=True, device=DEVICE)\n",
    "#print(slb)\n",
    "\n",
    "#training parameters\n",
    "\n",
    "#learning rate\n",
    "lr = 0.0001\n",
    "#number of epoch\n",
    "num_epoch = 150\n",
    "#peforming linear regression\n",
    "net = linreg\n",
    "#performing linear regression 2\n",
    "net2 = linreg\n",
    "#batch size\n",
    "batch_size = 20\n",
    "#using the weight decay within our sgd\n",
    "weight_decay = 1 - 1e-7\n",
    "\n",
    "acc = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for X, y in data_iter(batch_size, x_train.T, y_train.T):\n",
    "        l = cross_entropy((net2(net(X,tflw.float(),flb.float()),tslw.float(), slb.float())), y).to(DEVICE)\n",
    "        l.sum().backward()\n",
    "        sgd([tflw, flb], lr, batch_size, weight_decay)\n",
    "        sgd([tslw, slb], lr, batch_size, weight_decay)\n",
    "    with torch.no_grad():\n",
    "        soft = softmax(net2(net(x_test.T, tflw.float(), flb.float()), tslw.float(), slb.float())).to(DEVICE)\n",
    "        train_l = cross_entropy(net2(net(x_test.T, tflw.float(), flb.float()), tslw.float(), slb.float()),y_test.T).to(DEVICE)\n",
    "        a = accuracy(soft, y_test)\n",
    "        acc.append(a)\n",
    "        print(\"Epoch: \", epoch+1, \"\\nLoss:\", float(train_l.mean()),\"\\nAccuracy: \", a)\n",
    "\n",
    "plt.plot(acc)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Epoch vs Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop For CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072, 50000])\n",
      "torch.Size([10, 50000])\n",
      "torch.Size([3072, 10000])\n",
      "torch.Size([10, 10000])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Runs For Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 1\n",
    "\n",
    "Motivation For This Run: \n",
    "- This is the default run, we are testing the network with my version of the minimal number of layers and little attention to the optimial numbers for learning rate and regularization, a low amount of epoch so the network doesn't have much time to go over the data and learn, so overall a poorly optimized network as a baseline.\n",
    "\n",
    "Number of layers: \n",
    "- Input: 784\n",
    "- Hidden: 10\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 804\n",
    "\n",
    "Learning Rate:\n",
    "- 0.01\n",
    "\n",
    "Batch Size:\n",
    "- 100\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-7\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 5\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.1966\n",
    "\n",
    "GPU Memory Usage: \n",
    "- 1186MiB\n",
    "\n",
    "Acutal Training Time:\n",
    "- Time: 5.66s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training](one.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 2\n",
    "\n",
    "Motivation For This Run: \n",
    "- I have increased the number of epoch within the network so the model can make more passes through the network and hopefully learn more of the data and increase the accuracy of the network overall. \n",
    "\n",
    "Number of layers: \n",
    "- Input: 784\n",
    "- Hidden: 10\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 804\n",
    "\n",
    "Learning Rate:\n",
    "- 0.01\n",
    "\n",
    "Batch Size:\n",
    "- 100\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-7\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 10\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.58\n",
    "\n",
    "GPU Memory Usage: \n",
    "- 1189MiB\n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 11.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](two.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 3\n",
    "\n",
    "Motivation For This Run: \n",
    "- Keeping the epoch at 10, I want to see if we increaase the incrementation of the amount of hidden layers to 50, this should increase the time complexity but also we able to solve more complex problems which might be able to do better on this dataset.\n",
    "\n",
    "Number of layers: \n",
    "- Input: 784\n",
    "- Hidden: 50\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 814\n",
    "\n",
    "Learning Rate:\n",
    "- 0.01\n",
    "\n",
    "Batch Size:\n",
    "- 100\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-7\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 10\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.45sc\n",
    "\n",
    "GPU Memory Usage: \n",
    "- 1190MiB\n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 12.6s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](three.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 4\n",
    "\n",
    "Motivation For This Run: \n",
    "- By noticing the increase of the accuracy by just more hidden layer nodes, I will add 50 more hidden layer nodes, and increase the amount of epochs so the model has more iterations through the dataset. This should increase accuracy significantly.\n",
    "\n",
    "Number of layers: \n",
    "- Input: 784\n",
    "- Hidden: 100\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 894\n",
    "\n",
    "Learning Rate:\n",
    "- 0.01\n",
    "\n",
    "Batch Size:\n",
    "- 100\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-7\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 50\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.69\n",
    "\n",
    "GPU Memory Usage: \n",
    "- 1190MiB\n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 1min 3s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](four.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 5\n",
    "\n",
    "Motivation For This Run: \n",
    "- Within this run, I saw decent improvement in the last test run in accuracy, so for this run I want to decrease the batch size because since the batches will be smaller, it will be easier to prioritize certain data than combining it with a variety of different values that are included within a greater batch size. Also, we will see how a difference of batch size affect the memory utilization, especially after reading about it in our reflection for the week. Also, as a result might be a longer time to complete the run but reduce the loss marginally.\n",
    "\n",
    "Number of layers: \n",
    "- Input: 784\n",
    "- Hidden: 100\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 894\n",
    "\n",
    "Learning Rate:\n",
    "- 0.01\n",
    "\n",
    "Batch Size:\n",
    "- 50\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-7\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 50\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.7543\n",
    "\n",
    "GPU Memory Usage: \n",
    "- 1398MiB\n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 1min 53s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](five.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 6\n",
    "\n",
    "Motivation For This Run: \n",
    "- In this run I went to decrease the learning rate, a smaller learning rates will require me increase the amount of epoch but the idea of this run is to see if the model can learn and find a more optimial global minima. As a result, I do expect the time to be longer. Also, my prediction is I would have to substantially increase the epochs for this learning rate so I want to see how much the learning rate decreases by. \n",
    "\n",
    "\n",
    "Number of layers: \n",
    "- Input: 784\n",
    "- Hidden: 100\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 894\n",
    "\n",
    "Learning Rate:\n",
    "- 0.001\n",
    "\n",
    "Batch Size:\n",
    "- 50\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-7\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 100\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.63\n",
    "\n",
    "GPU Memory Usage: \n",
    "- ~979MiB\n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 3min 47s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](six.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 7\n",
    "\n",
    "Motivation For This Run: \n",
    "- In this run, I wanted to increase the amount of hidden nodes within the hidden layer to see if there is any improvement for accuracy and to see if the runtime increases significantly.\n",
    "\n",
    "\n",
    "Number of layers: \n",
    "- Input: 784\n",
    "- Hidden: 200\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 994\n",
    "\n",
    "Learning Rate:\n",
    "- 0.001\n",
    "\n",
    "Batch Size:\n",
    "- 50\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-7\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 100\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.6914\n",
    "\n",
    "GPU Memory Usage: \n",
    "- ~978MiB\n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 3min 50s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](seven.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 8\n",
    "\n",
    "Motivation For This Run: \n",
    "- For this run I wanted to see if I increase the number of epoch and the learning rate to see if the model can achieve a better accuracy\n",
    "\n",
    "\n",
    "Number of layers: \n",
    "- Input: 784\n",
    "- Hidden: 200\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 994\n",
    "\n",
    "Learning Rate:\n",
    "- 0.0001\n",
    "\n",
    "Batch Size:\n",
    "- 20\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-7\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 150\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.6979\n",
    "\n",
    "GPU Memory Usage: \n",
    "- ~999MiB\n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 18min 11s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 9\n",
    "\n",
    "Motivation For This Run: \n",
    "- I wanted to see if more hidden layer nodes would increase the accuracy but it got the same accuracy as the pervious run with only 200 for the amount of nodes more than 100 don't seem to work.\n",
    "\n",
    "\n",
    "Number of layers: \n",
    "- Input: 784\n",
    "- Hidden: 300\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 994\n",
    "\n",
    "Learning Rate:\n",
    "- 0.00001\n",
    "\n",
    "Batch Size:\n",
    "- 20\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-7\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 100\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.6956\n",
    "\n",
    "GPU Memory Usage: \n",
    "- ~989MiB\n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 9min 10s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 10\n",
    "\n",
    "Motivation For This Run: \n",
    "- Changing the learining rate back to a default value, keeping the amount of hidden layers  and changing the regularization value to see if there is the regularization can improve the accuracy. But by the end of my runs, my best accuracy was an 85.\n",
    "\n",
    "\n",
    "Number of layers: \n",
    "- Input: 784\n",
    "- Hidden: 300\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 994\n",
    "\n",
    "Learning Rate:\n",
    "- 0.05\n",
    "\n",
    "Batch Size:\n",
    "- 20\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-9\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 100\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.85\n",
    "\n",
    "GPU Memory Usage: \n",
    "- ~998MiB\n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 9min 4s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests Runs for Cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \n",
      "Loss: 11.19776439666748 \n",
      "Accuracy:  0.308\n",
      "Epoch:  2 \n",
      "Loss: 11.13414478302002 \n",
      "Accuracy:  0.3568\n",
      "Epoch:  3 \n",
      "Loss: 11.101150512695312 \n",
      "Accuracy:  0.3775\n",
      "Epoch:  4 \n",
      "Loss: 11.06143856048584 \n",
      "Accuracy:  0.3932\n",
      "Epoch:  5 \n",
      "Loss: 11.068934440612793 \n",
      "Accuracy:  0.3948\n",
      "Epoch:  6 \n",
      "Loss: 11.081104278564453 \n",
      "Accuracy:  0.3992\n",
      "Epoch:  7 \n",
      "Loss: 11.056868553161621 \n",
      "Accuracy:  0.4039\n",
      "Epoch:  8 \n",
      "Loss: 11.065499305725098 \n",
      "Accuracy:  0.404\n",
      "Epoch:  9 \n",
      "Loss: 11.046412467956543 \n",
      "Accuracy:  0.4062\n",
      "Epoch:  10 \n",
      "Loss: 11.054692268371582 \n",
      "Accuracy:  0.4028\n",
      "Epoch:  11 \n",
      "Loss: 11.062657356262207 \n",
      "Accuracy:  0.4059\n",
      "Epoch:  12 \n",
      "Loss: 11.024829864501953 \n",
      "Accuracy:  0.4063\n",
      "Epoch:  13 \n",
      "Loss: 11.053426742553711 \n",
      "Accuracy:  0.4048\n",
      "Epoch:  14 \n",
      "Loss: 11.0575590133667 \n",
      "Accuracy:  0.4045\n",
      "Epoch:  15 \n",
      "Loss: 11.035388946533203 \n",
      "Accuracy:  0.4082\n",
      "Epoch:  16 \n",
      "Loss: 11.034516334533691 \n",
      "Accuracy:  0.4064\n",
      "Epoch:  17 \n",
      "Loss: 11.029309272766113 \n",
      "Accuracy:  0.4112\n",
      "Epoch:  18 \n",
      "Loss: 11.034430503845215 \n",
      "Accuracy:  0.4083\n",
      "Epoch:  19 \n",
      "Loss: 11.035697937011719 \n",
      "Accuracy:  0.408\n",
      "Epoch:  20 \n",
      "Loss: 11.036958694458008 \n",
      "Accuracy:  0.412\n",
      "Epoch:  21 \n",
      "Loss: 11.020146369934082 \n",
      "Accuracy:  0.4111\n",
      "Epoch:  22 \n",
      "Loss: 11.031000137329102 \n",
      "Accuracy:  0.4072\n",
      "Epoch:  23 \n",
      "Loss: 11.024626731872559 \n",
      "Accuracy:  0.4105\n",
      "Epoch:  24 \n",
      "Loss: 11.057476043701172 \n",
      "Accuracy:  0.4122\n",
      "Epoch:  25 \n",
      "Loss: 11.035696983337402 \n",
      "Accuracy:  0.4132\n",
      "Epoch:  26 \n",
      "Loss: 11.023033142089844 \n",
      "Accuracy:  0.405\n",
      "Epoch:  27 \n",
      "Loss: 11.0355806350708 \n",
      "Accuracy:  0.4119\n",
      "Epoch:  28 \n",
      "Loss: 11.03377628326416 \n",
      "Accuracy:  0.4119\n",
      "Epoch:  29 \n",
      "Loss: 11.03298568725586 \n",
      "Accuracy:  0.409\n",
      "Epoch:  30 \n",
      "Loss: 11.037701606750488 \n",
      "Accuracy:  0.4083\n",
      "Epoch:  31 \n",
      "Loss: 11.028373718261719 \n",
      "Accuracy:  0.4109\n",
      "Epoch:  32 \n",
      "Loss: 11.027077674865723 \n",
      "Accuracy:  0.412\n",
      "Epoch:  33 \n",
      "Loss: 11.043010711669922 \n",
      "Accuracy:  0.4099\n",
      "Epoch:  34 \n",
      "Loss: 11.062723159790039 \n",
      "Accuracy:  0.4102\n",
      "Epoch:  35 \n",
      "Loss: 11.059487342834473 \n",
      "Accuracy:  0.4055\n",
      "Epoch:  36 \n",
      "Loss: 11.030075073242188 \n",
      "Accuracy:  0.4101\n",
      "Epoch:  37 \n",
      "Loss: 11.031126022338867 \n",
      "Accuracy:  0.4141\n",
      "Epoch:  38 \n",
      "Loss: 11.035055160522461 \n",
      "Accuracy:  0.4135\n",
      "Epoch:  39 \n",
      "Loss: 11.031964302062988 \n",
      "Accuracy:  0.4134\n",
      "Epoch:  40 \n",
      "Loss: 11.050448417663574 \n",
      "Accuracy:  0.4116\n",
      "Epoch:  41 \n",
      "Loss: 11.041973114013672 \n",
      "Accuracy:  0.413\n",
      "Epoch:  42 \n",
      "Loss: 11.042263984680176 \n",
      "Accuracy:  0.4094\n",
      "Epoch:  43 \n",
      "Loss: 11.030770301818848 \n",
      "Accuracy:  0.4099\n",
      "Epoch:  44 \n",
      "Loss: 11.04257583618164 \n",
      "Accuracy:  0.4113\n",
      "Epoch:  45 \n",
      "Loss: 11.045315742492676 \n",
      "Accuracy:  0.4117\n",
      "Epoch:  46 \n",
      "Loss: 11.032537460327148 \n",
      "Accuracy:  0.4088\n",
      "Epoch:  47 \n",
      "Loss: 11.038107872009277 \n",
      "Accuracy:  0.4113\n",
      "Epoch:  48 \n",
      "Loss: 11.027793884277344 \n",
      "Accuracy:  0.4126\n",
      "Epoch:  49 \n",
      "Loss: 11.057785034179688 \n",
      "Accuracy:  0.4129\n",
      "Epoch:  50 \n",
      "Loss: 11.039956092834473 \n",
      "Accuracy:  0.4091\n",
      "Epoch:  51 \n",
      "Loss: 11.025653839111328 \n",
      "Accuracy:  0.4105\n",
      "Epoch:  52 \n",
      "Loss: 11.034158706665039 \n",
      "Accuracy:  0.4097\n",
      "Epoch:  53 \n",
      "Loss: 11.039179801940918 \n",
      "Accuracy:  0.41\n",
      "Epoch:  54 \n",
      "Loss: 11.038948059082031 \n",
      "Accuracy:  0.4109\n",
      "Epoch:  55 \n",
      "Loss: 11.042457580566406 \n",
      "Accuracy:  0.4119\n",
      "Epoch:  56 \n",
      "Loss: 11.030983924865723 \n",
      "Accuracy:  0.4101\n",
      "Epoch:  57 \n",
      "Loss: 11.037443161010742 \n",
      "Accuracy:  0.4099\n",
      "Epoch:  58 \n",
      "Loss: 11.0303316116333 \n",
      "Accuracy:  0.4113\n",
      "Epoch:  59 \n",
      "Loss: 11.037200927734375 \n",
      "Accuracy:  0.4093\n",
      "Epoch:  60 \n",
      "Loss: 11.054567337036133 \n",
      "Accuracy:  0.4087\n",
      "Epoch:  61 \n",
      "Loss: 11.03018569946289 \n",
      "Accuracy:  0.4081\n",
      "Epoch:  62 \n",
      "Loss: 11.030442237854004 \n",
      "Accuracy:  0.4124\n",
      "Epoch:  63 \n",
      "Loss: 11.031371116638184 \n",
      "Accuracy:  0.4118\n",
      "Epoch:  64 \n",
      "Loss: 11.043790817260742 \n",
      "Accuracy:  0.4108\n",
      "Epoch:  65 \n",
      "Loss: 11.038068771362305 \n",
      "Accuracy:  0.4081\n",
      "Epoch:  66 \n",
      "Loss: 11.028640747070312 \n",
      "Accuracy:  0.4096\n",
      "Epoch:  67 \n",
      "Loss: 11.0458345413208 \n",
      "Accuracy:  0.4092\n",
      "Epoch:  68 \n",
      "Loss: 11.043596267700195 \n",
      "Accuracy:  0.409\n",
      "Epoch:  69 \n",
      "Loss: 11.036680221557617 \n",
      "Accuracy:  0.4099\n",
      "Epoch:  70 \n",
      "Loss: 11.064115524291992 \n",
      "Accuracy:  0.4061\n",
      "Epoch:  71 \n",
      "Loss: 11.042732238769531 \n",
      "Accuracy:  0.4096\n",
      "Epoch:  72 \n",
      "Loss: 11.060835838317871 \n",
      "Accuracy:  0.4091\n",
      "Epoch:  73 \n",
      "Loss: 11.04272174835205 \n",
      "Accuracy:  0.4072\n",
      "Epoch:  74 \n",
      "Loss: 11.051953315734863 \n",
      "Accuracy:  0.4099\n",
      "Epoch:  75 \n",
      "Loss: 11.035724639892578 \n",
      "Accuracy:  0.4126\n",
      "Epoch:  76 \n",
      "Loss: 11.043286323547363 \n",
      "Accuracy:  0.4069\n",
      "Epoch:  77 \n",
      "Loss: 11.061065673828125 \n",
      "Accuracy:  0.4055\n",
      "Epoch:  78 \n",
      "Loss: 11.057509422302246 \n",
      "Accuracy:  0.4083\n",
      "Epoch:  79 \n",
      "Loss: 11.039313316345215 \n",
      "Accuracy:  0.4097\n",
      "Epoch:  80 \n",
      "Loss: 11.04122257232666 \n",
      "Accuracy:  0.4092\n",
      "Epoch:  81 \n",
      "Loss: 11.059467315673828 \n",
      "Accuracy:  0.4075\n",
      "Epoch:  82 \n",
      "Loss: 11.05073070526123 \n",
      "Accuracy:  0.4094\n",
      "Epoch:  83 \n",
      "Loss: 11.03563117980957 \n",
      "Accuracy:  0.4087\n",
      "Epoch:  84 \n",
      "Loss: 11.066446304321289 \n",
      "Accuracy:  0.4085\n",
      "Epoch:  85 \n",
      "Loss: 11.033523559570312 \n",
      "Accuracy:  0.4108\n",
      "Epoch:  86 \n",
      "Loss: 11.044504165649414 \n",
      "Accuracy:  0.4095\n",
      "Epoch:  87 \n",
      "Loss: 11.039185523986816 \n",
      "Accuracy:  0.4072\n",
      "Epoch:  88 \n",
      "Loss: 11.03890609741211 \n",
      "Accuracy:  0.4096\n",
      "Epoch:  89 \n",
      "Loss: 11.031466484069824 \n",
      "Accuracy:  0.4101\n",
      "Epoch:  90 \n",
      "Loss: 11.0484037399292 \n",
      "Accuracy:  0.4069\n",
      "Epoch:  91 \n",
      "Loss: 11.039982795715332 \n",
      "Accuracy:  0.4063\n",
      "Epoch:  92 \n",
      "Loss: 11.03979206085205 \n",
      "Accuracy:  0.4076\n",
      "Epoch:  93 \n",
      "Loss: 11.045453071594238 \n",
      "Accuracy:  0.4072\n",
      "Epoch:  94 \n",
      "Loss: 11.04494571685791 \n",
      "Accuracy:  0.4089\n",
      "Epoch:  95 \n",
      "Loss: 11.045125961303711 \n",
      "Accuracy:  0.4102\n",
      "Epoch:  96 \n",
      "Loss: 11.044713020324707 \n",
      "Accuracy:  0.4075\n",
      "Epoch:  97 \n",
      "Loss: 11.043591499328613 \n",
      "Accuracy:  0.4092\n",
      "Epoch:  98 \n",
      "Loss: 11.044109344482422 \n",
      "Accuracy:  0.4076\n",
      "Epoch:  99 \n",
      "Loss: 11.059456825256348 \n",
      "Accuracy:  0.4081\n",
      "Epoch:  100 \n",
      "Loss: 11.06820297241211 \n",
      "Accuracy:  0.405\n",
      "CPU times: user 14min 39s, sys: 14 s, total: 14min 53s\n",
      "Wall time: 14min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "hidden_layers = 250\n",
    "flw = np.random.rand(3072, hidden_layers) * 0.001\n",
    "\n",
    "tflw = torch.tensor(flw, requires_grad=True, device=DEVICE)\n",
    "flb = torch.zeros(hidden_layers, requires_grad=True, device=DEVICE)\n",
    "\n",
    "slw = np.random.rand(hidden_layers, 10) * 0.001\n",
    "tslw = torch.tensor(slw, requires_grad=True, device=DEVICE)\n",
    "slb = torch.zeros(10, requires_grad=True, device=DEVICE)\n",
    "\n",
    "#learning rate\n",
    "lr = 0.05\n",
    "#number of epoch\n",
    "num_epoch = 100\n",
    "#peforming linear regression\n",
    "net = linreg\n",
    "#performing linear regression 2\n",
    "net2 = linreg\n",
    "#batch size\n",
    "batch_size = 10\n",
    "#using the weight decay within our sgd\n",
    "weight_decay = 1 - 1e-10\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for X, y in data_iter(batch_size, x_train.T, y_train.T):\n",
    "        l = cross_entropy((net2(net(X,tflw.float(),flb.float()),tslw.float(), slb.float())), y).to(DEVICE)\n",
    "        l.sum().backward()\n",
    "        sgd([tflw, flb], lr, batch_size, weight_decay)\n",
    "        sgd([tslw, slb], lr, batch_size, weight_decay)\n",
    "    with torch.no_grad():\n",
    "        soft = softmax(net2(net(x_test.T, tflw.float(), flb.float()), tslw.float(), slb.float())).to(DEVICE)\n",
    "        train_l = cross_entropy(net2(net(x_test.T, tflw.float(), flb.float()), tslw.float(), slb.float()),y_test.T).to(DEVICE)\n",
    "        print(\"Epoch: \", epoch+1, \"\\nLoss:\", float(train_l.mean()),\"\\nAccuracy: \", accuracy(soft, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 1\n",
    "\n",
    "Motivation For This Run: \n",
    "- This is a baseline for the Test runs, this is me just guessing for the network and not having anything in specific in mind before doing this test run.\n",
    "\n",
    "Number of layers: \n",
    "- Input: 3072\n",
    "- Hidden: 100\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 3182\n",
    "\n",
    "Learning Rate:\n",
    "- 0.05\n",
    "\n",
    "Batch Size:\n",
    "- 50\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-7\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 200\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.406\n",
    "\n",
    "GPU Memory Usage: \n",
    "- ~2216MiB(nvidia-smi doesn't update beacuase the run wasn't more than ) \n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 6min 49s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 2\n",
    "\n",
    "Motivation For This Run: \n",
    "- I want to see if more hidden layer nodes increase the accuracy of the model.\n",
    "\n",
    "Number of layers: \n",
    "- Input: 3072\n",
    "- Hidden: 200\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 3282\n",
    "\n",
    "Learning Rate:\n",
    "- 0.05\n",
    "\n",
    "Batch Size:\n",
    "- 50\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-7\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 200\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: \n",
    "\n",
    "GPU Memory Usage: \n",
    "- ~2216MiB(nvidia-smi doesn't update beacuase the run wasn't more than ) \n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 6min 56s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 3\n",
    "\n",
    "Motivation For This Run: \n",
    "- I increased the batch size to see if I can get the network to finish a bit faster and still get the same accuracy as the previous run.\n",
    "\n",
    "Number of layers: \n",
    "- Input: 3072\n",
    "- Hidden: 200\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 3282\n",
    "\n",
    "Learning Rate:\n",
    "- 0.05\n",
    "\n",
    "Batch Size:\n",
    "- 50\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-10\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 200\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.4111\n",
    "\n",
    "GPU Memory Usage: \n",
    "- ~2216MiB(nvidia-smi doesn't update beacuase the run wasn't more than) \n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 10min 28s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run: 4\n",
    "\n",
    "Motivation For This Run: \n",
    "- I changed the multiplication of the weights to see if smaller weights would help the model learn better.\n",
    "\n",
    "Number of layers: \n",
    "- Input: 3072\n",
    "- Hidden: 200\n",
    "- Output: 10\n",
    "\n",
    "Number of Nodes in Fully Connected Layer:\n",
    "- Number of nodes: 3282\n",
    "\n",
    "Learning Rate:\n",
    "- 0.05\n",
    "\n",
    "Batch Size:\n",
    "- 30\n",
    "\n",
    "Regularization Constant:\n",
    "- 1 - 1e-10\n",
    "\n",
    "How long the network trained(epoch, or samples seen):\n",
    "- Epoch: 100\n",
    "\n",
    "Test Accuracy: \n",
    "- Accuracy: 0.4111\n",
    "\n",
    "GPU Memory Usage: \n",
    "- ~2216MiB(nvidia-smi doesn't update beacuase the run wasn't more than ) \n",
    "\n",
    "Actual Training Time:\n",
    "- Time: 10min 28s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### didn't have time to run more tests because they would have taken to long with the Rosie Problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
