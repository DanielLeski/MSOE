{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import warnings\n",
    "import os.path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "# For simple regression problem\n",
    "TRAINING_POINTS = 1000\n",
    "\n",
    "# For fashion-MNIST and similar problems\n",
    "DATA_ROOT = '/data/cs3450/data/'\n",
    "FASHION_MNIST_TRAINING = '/data/cs3450/data/fashion_mnist_flattened_training.npz'\n",
    "FASHION_MNIST_TESTING = '/data/cs3450/data/fashion_mnist_flattened_testing.npz'\n",
    "CIFAR10_TRAINING = '/data/cs3450/data/cifar10_flattened_training.npz'\n",
    "CIFAR10_TESTING = '/data/cs3450/data/cifar10_flattened_testing.npz'\n",
    "CIFAR100_TRAINING = '/data/cs3450/data/cifar100_flattened_training.npz'\n",
    "CIFAR100_TESTING = '/data/cs3450/data/cifar100_flattened_testing.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\n",
    "       https://d2l.ai/chapter_deep-learning-computation/use-gpu.html\n",
    "    \"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "DEVICE=try_gpu()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_linear_training_data():\n",
    "    \"\"\"\n",
    "    This method simply rotates points in a 2D space.\n",
    "    Be sure to use L2 regression in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_folded_training_data():\n",
    "    \"\"\"\n",
    "    This method introduces a single non-linear fold into the sort of data created by create_linear_training_data. Be sure to REMOVE the final softmax layer before testing on this data!\n",
    "    Be sure to use L2 regression in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    x2 *= 2 * ((x2 > 0).float() - 0.5)\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def create_square():\n",
    "    \"\"\"\n",
    "    This is the square example that we looked at in class.\n",
    "    insideness is true if the points are inside the square.\n",
    "    :return: (points, insideness) the dataset. points is a 2xN array of points and insideness is true if the point is inside the square.\n",
    "    \"\"\"\n",
    "    win_x = [2,2,3,3]\n",
    "    win_y = [1,2,2,1]\n",
    "    win = torch.tensor([win_x,win_y],dtype=torch.float32)\n",
    "    win_rot = torch.cat((win[:,1:],win[:,0:1]),axis=1)\n",
    "    t = win_rot - win # edges tangent along side of poly\n",
    "    rotation = torch.tensor([[0, 1],[-1,0]],dtype=torch.float32)\n",
    "    normal = rotation @ t # normal vectors to each side of poly\n",
    "        # torch.matmul(rotation,t) # Same thing\n",
    "\n",
    "    points = torch.rand((2,2000),dtype = torch.float32)\n",
    "    points = 4*points\n",
    "\n",
    "    vectors = points[:,np.newaxis,:] - win[:,:,np.newaxis] # reshape to fill origin\n",
    "    insideness = (normal[:,:,np.newaxis] * vectors).sum(axis=0)\n",
    "    insideness = insideness.T\n",
    "    insideness = insideness > 0\n",
    "    insideness = insideness.all(axis=1)\n",
    "    return points, insideness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_flattened(train=True,dataset='Fashion-MNIST',download=False):\n",
    "    \"\"\"\n",
    "    :param train: True for training, False for testing\n",
    "    :param dataset: 'Fashion-MNIST', 'CIFAR-10', or 'CIFAR-100'\n",
    "    :param download: True to download. Keep to false afterwords to avoid unneeded downloads.\n",
    "    :return: (x,y) the dataset. x is a numpy array where columns are training samples and\n",
    "             y is a numpy array where columns are one-hot labels for the training sample.\n",
    "    \"\"\"\n",
    "    if dataset == 'Fashion-MNIST':\n",
    "        if train:\n",
    "            path = FASHION_MNIST_TRAINING\n",
    "        else:\n",
    "            path = FASHION_MNIST_TESTING\n",
    "        num_labels = 10\n",
    "    elif dataset == 'CIFAR-10':\n",
    "        if train:\n",
    "            path = CIFAR10_TRAINING\n",
    "        else:\n",
    "            path = CIFAR10_TESTING\n",
    "        num_labels = 10\n",
    "    elif dataset == 'CIFAR-100':\n",
    "        if train:\n",
    "            path = CIFAR100_TRAINING\n",
    "        else:\n",
    "            path = CIFAR100_TESTING\n",
    "        num_labels = 100\n",
    "    else:\n",
    "        raise ValueError('Unknown dataset: '+str(dataset))\n",
    "\n",
    "    if os.path.isfile(path):\n",
    "        print('Loading cached flattened data for',dataset,'training' if train else 'testing')\n",
    "        data = np.load(path)\n",
    "        x = torch.tensor(data['x'],dtype=torch.float32)\n",
    "        y = torch.tensor(data['y'],dtype=torch.float32)\n",
    "        pass\n",
    "    else:\n",
    "        class ToTorch(object):\n",
    "            \"\"\"Like ToTensor, only to a numpy array\"\"\"\n",
    "\n",
    "            def __call__(self, pic):\n",
    "                return torchvision.transforms.functional.to_tensor(pic)\n",
    "\n",
    "        if dataset == 'Fashion-MNIST':\n",
    "            data = torchvision.datasets.FashionMNIST(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        elif dataset == 'CIFAR-10':\n",
    "            data = torchvision.datasets.CIFAR10(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        elif dataset == 'CIFAR-100':\n",
    "            data = torchvision.datasets.CIFAR100(\n",
    "                root=DATA_ROOT, train=train, transform=ToTorch(), download=download)\n",
    "        else:\n",
    "            raise ValueError('This code should be unreachable because of a previous check.')\n",
    "        x = torch.zeros((len(data[0][0].flatten()), len(data)),dtype=torch.float32)\n",
    "        for index, image in enumerate(data):\n",
    "            x[:, index] = data[index][0].flatten()\n",
    "        labels = torch.tensor([sample[1] for sample in data])\n",
    "        y = torch.zeros((num_labels, len(labels)), dtype=torch.float32)\n",
    "        y[labels, torch.arange(len(labels))] = 1\n",
    "        np.savez(path, x=x.detach().numpy(), y=y.detach().numpy())\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached flattened data for Fashion-MNIST training\n"
     ]
    }
   ],
   "source": [
    "# TODO: Select your datasource.\n",
    "dataset = 'Fashion-MNIST'\n",
    "# dataset = 'CIFAR-10'\n",
    "# dataset = 'CIFAR-100'\n",
    "\n",
    "#x_train, y_train = create_linear_training_data()\n",
    "#x_train, y_train = create_folded_training_data()\n",
    "#points_train, insideness_train = create_square()\n",
    "x_train, y_train = load_dataset_flattened(train=True, dataset=dataset, download=False)\n",
    "# Move selected datasets to GPU\n",
    "x_train = x_train.to(DEVICE)\n",
    "y_train = y_train.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the accuracy of your network\n",
    "x_test, y_test = create_linear_training_data()\n",
    "# x_test, y_test = load_dataset_flattened(train=False, dataset=dataset, download=False)\n",
    "\n",
    "# Move the selected datasets to the GPU\n",
    "x_test = x_test.to(DEVICE)\n",
    "y_test = y_test.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0848,  1.6209,  1.7899,  ..., -0.3476,  0.6443,  1.8259],\n",
       "        [-1.6644, -1.0515,  1.5957,  ..., -0.5752, -1.0943, -0.3921]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6644,  1.0515, -1.5957,  ...,  0.5752,  1.0943,  0.3921],\n",
       "        [-1.0848,  1.6209,  1.7899,  ..., -0.3476,  0.6443,  1.8259]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3 Training a Neural Network using Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Forward Propagation Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case #1 (In PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input\n",
    "x = torch.tensor([6,7])\n",
    "#weights\n",
    "w = torch.tensor(np.array([[0,1,2], [3,4,5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using regularization on the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21, 34, 47])\n"
     ]
    }
   ],
   "source": [
    "#hidden layer result\n",
    "h = torch.matmul(x,w)\n",
    "h = relu(h)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([128, 434])\n"
     ]
    }
   ],
   "source": [
    "#Output layer result\n",
    "o = torch.matmul(w,h)\n",
    "o = relu(o)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learningrate = 0.1\n",
    "s = (learningrate/2)*((torch.pow(torch.norm(w.float(),p='fro'),2) + torch.pow(torch.norm(w.float(), p='fro'),2)))\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case #2 (In PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.tensor(np.array([20,44]))\n",
    "ww = torch.tensor(np.array([[-1,2,3], [-5,3,7]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0, 172, 368])\n"
     ]
    }
   ],
   "source": [
    "hh = torch.matmul(ww,xx)\n",
    "hh = relu(hh)\n",
    "print(hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1448, 3092])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ww2 = torch.tensor(np.array([[-1,2,3], [-5,3,7]]))\n",
    "oo = torch.matmul(ww2,hh)\n",
    "oo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.7000)\n"
     ]
    }
   ],
   "source": [
    "learningrate = 0.1\n",
    "s = (learningrate/2)*((torch.pow(torch.norm(ww.float(),p='fro'),2) + torch.pow(torch.norm(ww2.float(), p='fro'),2)))\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the batch sizes (Implemention of MiniBatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    n = len(features)\n",
    "    iss = list(range(n))\n",
    "    random.shuffle(iss)\n",
    "    for i in range(0, n, batch_size):\n",
    "        b = torch.tensor(iss[i:min(i + batch_size, n)])\n",
    "        yield features[b], labels[b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Sqaure Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2 /2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X,w,b):\n",
    "    return relu(torch.matmul(X,w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size, weight_decay):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param *= weight_decay\n",
    "            param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    m = torch.max(x,0)\n",
    "    v = torch.exp(x - m.values)\n",
    "    b = torch.sum(v,0)\n",
    "    t = v / b\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(X,y):\n",
    "    x = -torch.sum((X * torch.log(y + 1e-7)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop For Backprop Using Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Tensor #1:  tensor([[0.0402, 0.0170, 0.0461,  ..., 0.0668, 0.0202, 0.0208],\n",
      "        [0.0933, 0.0818, 0.0280,  ..., 0.0799, 0.0311, 0.0696],\n",
      "        [0.0522, 0.0434, 0.0980,  ..., 0.0619, 0.0890, 0.0988],\n",
      "        ...,\n",
      "        [0.0707, 0.0941, 0.0571,  ..., 0.0335, 0.0206, 0.0898],\n",
      "        [0.0014, 0.0337, 0.0062,  ..., 0.0462, 0.0870, 0.0963],\n",
      "        [0.0155, 0.0062, 0.0817,  ..., 0.0679, 0.0252, 0.0240]],\n",
      "       dtype=torch.float64, requires_grad=True)\n",
      "Weight Tensor #2:  tensor([[8.9243e-02, 8.5325e-02, 2.8366e-02, 1.9904e-02, 4.3965e-02, 6.0767e-02,\n",
      "         2.4935e-02, 5.6028e-02, 2.7772e-02, 4.9869e-02],\n",
      "        [2.4205e-02, 3.9914e-02, 1.9123e-02, 4.5479e-02, 6.1670e-02, 4.7722e-02,\n",
      "         6.1965e-02, 2.1921e-02, 9.1088e-02, 9.7733e-02],\n",
      "        [6.9839e-02, 3.5937e-02, 6.3078e-02, 8.1281e-02, 1.8291e-02, 7.6320e-02,\n",
      "         9.1307e-02, 7.4318e-02, 9.9393e-03, 4.1007e-02],\n",
      "        [8.4259e-05, 5.1603e-02, 7.9588e-02, 1.7429e-03, 2.0042e-02, 6.8784e-03,\n",
      "         4.6070e-02, 2.5874e-02, 9.1097e-02, 1.7232e-02],\n",
      "        [4.5161e-02, 9.6717e-03, 4.1519e-03, 2.6595e-02, 2.2843e-03, 3.3375e-02,\n",
      "         4.6569e-02, 2.8638e-02, 2.4851e-02, 2.3231e-03],\n",
      "        [3.6781e-02, 2.3855e-02, 3.9183e-04, 4.2244e-02, 2.8186e-03, 5.4465e-02,\n",
      "         7.7797e-02, 9.9132e-02, 8.8059e-02, 6.6428e-02],\n",
      "        [4.9807e-02, 1.0969e-02, 6.4612e-02, 6.2576e-02, 3.9256e-03, 7.2436e-02,\n",
      "         5.7914e-02, 9.8986e-02, 6.3468e-02, 1.4655e-02],\n",
      "        [7.1237e-03, 5.4231e-02, 9.6538e-02, 7.8427e-02, 7.6524e-03, 8.0269e-02,\n",
      "         7.2520e-02, 3.8564e-02, 2.1815e-02, 9.0902e-02],\n",
      "        [2.6737e-02, 6.9836e-02, 2.4042e-05, 3.6320e-02, 8.2660e-02, 7.4804e-02,\n",
      "         6.1966e-02, 5.4811e-02, 2.4560e-02, 5.0456e-02],\n",
      "        [1.4555e-02, 3.0680e-02, 3.4984e-02, 3.3592e-02, 5.9871e-02, 9.6710e-02,\n",
      "         2.4989e-02, 3.8997e-02, 8.0833e-02, 9.6756e-03],\n",
      "        [8.2164e-02, 7.7908e-02, 6.7653e-02, 8.5897e-02, 9.2233e-02, 9.9055e-02,\n",
      "         7.4224e-02, 6.7278e-02, 4.3474e-02, 4.1157e-02],\n",
      "        [9.9001e-02, 1.3984e-02, 9.1287e-02, 5.4034e-02, 9.4356e-02, 7.1050e-02,\n",
      "         1.2439e-02, 5.2529e-02, 1.2799e-02, 2.7875e-03],\n",
      "        [9.9643e-02, 2.5479e-02, 8.0536e-02, 2.9339e-02, 6.4994e-02, 5.7061e-02,\n",
      "         8.3780e-02, 2.5237e-02, 3.4401e-02, 9.8298e-02],\n",
      "        [7.9765e-02, 8.3867e-03, 4.3052e-03, 1.0287e-02, 1.3662e-02, 1.7961e-02,\n",
      "         2.0579e-02, 7.7720e-02, 2.5106e-02, 8.8543e-02],\n",
      "        [4.5063e-02, 9.1218e-02, 3.5119e-02, 7.9563e-02, 3.5899e-02, 3.5443e-02,\n",
      "         1.1358e-02, 2.8653e-02, 7.2536e-02, 1.1099e-03],\n",
      "        [7.9414e-03, 9.7859e-02, 2.7958e-02, 9.5955e-02, 4.6047e-02, 5.0811e-02,\n",
      "         2.5623e-02, 1.1593e-02, 5.9384e-02, 4.7436e-02],\n",
      "        [4.4427e-02, 5.9003e-02, 6.4358e-02, 6.7963e-02, 6.6460e-02, 7.6841e-02,\n",
      "         1.1965e-02, 4.1076e-02, 7.3021e-02, 5.2047e-02],\n",
      "        [3.1970e-02, 9.3452e-02, 6.6855e-02, 8.9206e-02, 1.9422e-02, 7.9571e-02,\n",
      "         7.7966e-02, 1.3584e-02, 7.4881e-02, 8.2192e-03],\n",
      "        [4.3451e-02, 8.2773e-02, 9.6352e-02, 3.8228e-02, 9.1282e-02, 2.9107e-02,\n",
      "         5.2725e-03, 4.9675e-02, 4.8872e-02, 2.7357e-02],\n",
      "        [9.7480e-02, 5.8030e-02, 3.3989e-02, 1.3774e-02, 1.0340e-04, 6.1162e-04,\n",
      "         1.8852e-02, 8.0869e-02, 3.5832e-02, 5.7905e-02],\n",
      "        [9.0802e-02, 4.3309e-02, 3.8052e-02, 6.9708e-02, 4.3632e-02, 5.8900e-02,\n",
      "         7.3361e-02, 1.9479e-02, 6.4514e-02, 8.9717e-02],\n",
      "        [1.6120e-02, 2.6362e-02, 8.3562e-02, 1.5166e-03, 5.1836e-02, 2.5244e-02,\n",
      "         1.7512e-02, 8.9674e-02, 5.0313e-02, 7.3495e-02],\n",
      "        [1.7916e-02, 8.1481e-02, 8.3205e-02, 3.6390e-02, 4.0199e-02, 2.6496e-02,\n",
      "         1.3362e-02, 8.0215e-02, 2.7394e-03, 1.8791e-02],\n",
      "        [1.6311e-02, 2.5915e-02, 3.3523e-02, 6.5365e-02, 9.6526e-02, 2.6433e-02,\n",
      "         6.4423e-02, 9.6694e-02, 2.1546e-02, 3.8351e-02],\n",
      "        [6.6894e-02, 9.2625e-03, 6.2982e-02, 3.0236e-02, 1.2759e-02, 6.3069e-02,\n",
      "         1.5586e-02, 5.1239e-02, 9.8769e-02, 4.5195e-02],\n",
      "        [3.6760e-02, 6.5700e-02, 9.2597e-02, 5.2394e-03, 8.0115e-02, 4.0747e-02,\n",
      "         9.2391e-02, 2.5865e-02, 5.3926e-02, 1.0706e-02],\n",
      "        [1.7825e-02, 9.6885e-02, 6.5085e-03, 8.7404e-02, 4.1083e-02, 2.0677e-02,\n",
      "         7.2425e-02, 7.9634e-03, 5.5953e-03, 2.7240e-02],\n",
      "        [9.4996e-02, 7.3150e-02, 4.4680e-02, 6.1519e-03, 9.7217e-03, 7.6537e-02,\n",
      "         7.5368e-04, 5.9224e-02, 5.2365e-02, 9.1050e-03],\n",
      "        [6.2558e-03, 9.2737e-02, 3.4680e-02, 6.0713e-02, 3.4592e-02, 3.3512e-02,\n",
      "         1.6604e-02, 4.3974e-02, 9.0219e-02, 5.3104e-02],\n",
      "        [7.6579e-03, 8.3328e-02, 4.8115e-02, 3.1182e-02, 4.4979e-02, 7.3342e-02,\n",
      "         2.3467e-02, 5.4124e-02, 1.5114e-02, 9.7943e-02],\n",
      "        [2.0905e-02, 4.6706e-02, 8.1086e-02, 2.3456e-02, 2.6277e-02, 7.6187e-02,\n",
      "         1.3660e-02, 3.5370e-02, 2.0536e-02, 8.3270e-02],\n",
      "        [1.0013e-03, 8.6811e-02, 1.5241e-02, 1.1729e-02, 2.6268e-02, 1.9622e-02,\n",
      "         1.9809e-03, 8.3406e-02, 7.7946e-02, 7.1027e-02],\n",
      "        [6.5124e-02, 4.2557e-02, 9.7697e-02, 5.8046e-02, 7.1296e-02, 1.0713e-02,\n",
      "         7.4525e-02, 3.9124e-03, 9.4459e-02, 8.7382e-02],\n",
      "        [4.5774e-02, 4.9108e-02, 3.9703e-02, 8.1284e-02, 7.4351e-02, 5.0110e-02,\n",
      "         8.0941e-02, 8.8745e-02, 1.8218e-02, 5.6506e-02],\n",
      "        [4.8356e-02, 3.9075e-02, 1.3402e-02, 5.9959e-02, 6.4990e-02, 9.2342e-02,\n",
      "         6.5433e-02, 2.5965e-02, 9.2776e-02, 5.7604e-02],\n",
      "        [2.1705e-02, 2.2298e-02, 9.5189e-03, 7.8010e-02, 9.4826e-02, 2.3072e-02,\n",
      "         9.4223e-02, 8.5566e-03, 7.5620e-02, 4.5425e-02],\n",
      "        [5.4095e-02, 6.1901e-02, 2.4332e-02, 5.4465e-02, 1.1290e-02, 1.1827e-02,\n",
      "         4.4360e-02, 7.4652e-02, 8.8381e-02, 5.2148e-02],\n",
      "        [8.7775e-02, 7.2721e-02, 8.6852e-02, 5.1222e-02, 5.0993e-02, 8.6919e-02,\n",
      "         2.5938e-02, 9.8635e-02, 9.8448e-02, 7.7508e-02],\n",
      "        [8.9838e-02, 5.4865e-02, 6.5663e-02, 1.8894e-02, 7.9714e-02, 4.0922e-02,\n",
      "         7.7730e-02, 9.5749e-02, 8.2730e-02, 6.4017e-03],\n",
      "        [9.7323e-02, 1.5237e-03, 5.3095e-02, 3.2070e-02, 4.0534e-02, 2.5458e-02,\n",
      "         7.5694e-02, 4.1928e-02, 9.5726e-02, 9.9446e-02],\n",
      "        [1.7228e-02, 9.3207e-02, 5.2831e-02, 2.8034e-02, 9.4475e-02, 6.2116e-02,\n",
      "         6.8846e-02, 4.8559e-02, 1.5991e-02, 4.6571e-02],\n",
      "        [4.0344e-02, 4.2952e-02, 2.0516e-02, 8.7618e-04, 4.5674e-02, 7.3478e-02,\n",
      "         7.7039e-02, 9.4337e-02, 3.8687e-02, 6.8856e-02],\n",
      "        [4.5324e-02, 7.5828e-02, 8.7353e-02, 7.7659e-02, 5.0862e-03, 9.5620e-02,\n",
      "         1.4339e-02, 1.8739e-02, 1.6347e-02, 9.2578e-02],\n",
      "        [1.4574e-02, 6.3571e-02, 9.8000e-02, 8.0855e-03, 7.5801e-02, 3.3108e-02,\n",
      "         9.0431e-02, 6.3772e-02, 1.2992e-02, 4.8524e-02],\n",
      "        [3.9446e-02, 6.0354e-02, 8.1220e-02, 7.1441e-02, 1.3940e-02, 4.4766e-02,\n",
      "         4.8642e-02, 5.8914e-02, 7.8288e-02, 2.9430e-02],\n",
      "        [8.6214e-03, 9.1695e-02, 3.6471e-02, 2.9530e-02, 6.8981e-02, 8.5992e-02,\n",
      "         8.9779e-02, 4.6777e-02, 2.7751e-02, 5.5810e-02],\n",
      "        [6.0259e-02, 9.8840e-02, 6.9357e-02, 6.8171e-02, 9.1948e-02, 2.3268e-02,\n",
      "         2.1603e-02, 4.1632e-02, 5.0514e-03, 8.3005e-02],\n",
      "        [8.4604e-02, 6.3427e-02, 6.7395e-02, 3.8766e-02, 4.5030e-02, 7.9500e-02,\n",
      "         8.1754e-02, 8.5484e-02, 5.3481e-02, 1.7240e-02],\n",
      "        [2.1648e-02, 7.8030e-02, 4.8353e-02, 1.6278e-02, 8.1293e-02, 8.3437e-02,\n",
      "         9.7962e-02, 2.4954e-02, 6.6927e-02, 2.4086e-02],\n",
      "        [8.8147e-02, 3.6029e-02, 1.8135e-02, 8.7496e-02, 4.4246e-02, 7.8696e-02,\n",
      "         1.0346e-02, 2.5827e-02, 6.3883e-02, 5.9795e-02],\n",
      "        [4.2427e-02, 8.0748e-02, 2.3724e-02, 7.0842e-02, 7.9717e-03, 7.4450e-02,\n",
      "         6.5564e-02, 5.5887e-02, 9.4343e-02, 2.5249e-02],\n",
      "        [8.2965e-02, 5.2787e-02, 9.0880e-02, 9.3313e-03, 9.5395e-02, 2.3001e-02,\n",
      "         5.8606e-02, 5.2604e-02, 2.8829e-02, 4.4844e-02],\n",
      "        [7.5373e-02, 5.8659e-02, 8.3217e-02, 3.0294e-02, 1.5465e-02, 9.7235e-02,\n",
      "         9.0097e-02, 1.2584e-02, 3.7812e-02, 2.0986e-02],\n",
      "        [6.2757e-02, 8.6233e-02, 3.2653e-02, 6.4727e-02, 2.9094e-02, 7.9240e-02,\n",
      "         4.6915e-02, 5.9064e-02, 8.5541e-02, 2.5867e-02],\n",
      "        [8.3088e-02, 3.6191e-03, 9.6226e-02, 9.7140e-02, 1.8743e-02, 6.7154e-02,\n",
      "         7.3063e-02, 5.3272e-02, 9.5087e-02, 8.4007e-03],\n",
      "        [3.6614e-02, 1.6164e-02, 3.2005e-02, 2.2019e-02, 4.5215e-02, 2.5311e-02,\n",
      "         4.9169e-02, 8.8047e-02, 8.5877e-02, 7.0296e-02],\n",
      "        [6.6258e-02, 7.1921e-02, 4.6996e-04, 3.1709e-02, 4.7248e-02, 1.8276e-02,\n",
      "         5.1878e-02, 4.0013e-02, 4.0822e-02, 2.9412e-02],\n",
      "        [7.9158e-02, 7.8964e-02, 2.6366e-02, 4.0989e-02, 8.5211e-02, 5.8979e-02,\n",
      "         6.3634e-02, 7.7511e-02, 9.3771e-02, 2.9751e-02],\n",
      "        [3.5791e-02, 8.3197e-02, 1.3967e-02, 3.8239e-02, 6.8614e-02, 6.2982e-02,\n",
      "         1.5716e-02, 9.6909e-02, 5.4688e-02, 1.0143e-02],\n",
      "        [2.2738e-02, 7.1962e-02, 4.1720e-02, 9.8074e-02, 3.0422e-02, 7.8632e-02,\n",
      "         6.9379e-02, 7.8412e-02, 3.3214e-02, 3.8193e-02],\n",
      "        [8.4318e-02, 4.0428e-02, 7.1907e-02, 7.5641e-02, 5.2517e-02, 7.0637e-02,\n",
      "         4.3186e-02, 7.1572e-02, 8.2483e-02, 7.3245e-02],\n",
      "        [6.4367e-02, 7.2420e-02, 5.3332e-02, 3.0389e-02, 9.1791e-05, 9.2049e-02,\n",
      "         6.1386e-02, 3.8258e-02, 5.3341e-03, 2.5180e-02],\n",
      "        [6.8853e-02, 2.4451e-02, 4.7615e-02, 1.7190e-02, 2.3233e-02, 4.4291e-02,\n",
      "         9.0463e-02, 3.1879e-03, 2.9567e-02, 4.2722e-02],\n",
      "        [9.3200e-02, 9.1946e-02, 1.3240e-03, 7.2203e-02, 5.6411e-02, 2.7896e-02,\n",
      "         4.8813e-02, 1.0541e-02, 5.9161e-02, 3.2969e-02],\n",
      "        [3.9129e-02, 6.5077e-02, 9.2202e-02, 7.9277e-03, 8.4559e-02, 4.4761e-02,\n",
      "         1.0281e-02, 5.0767e-02, 4.5014e-03, 2.5536e-02],\n",
      "        [1.7980e-02, 7.9503e-02, 8.6624e-02, 4.0456e-02, 5.6612e-02, 2.2652e-02,\n",
      "         1.5155e-02, 3.6574e-02, 3.1059e-02, 8.5428e-02],\n",
      "        [8.6148e-02, 6.1586e-02, 8.9829e-02, 9.3837e-02, 1.8903e-02, 7.0614e-02,\n",
      "         8.2101e-02, 4.5533e-02, 7.1265e-02, 1.3104e-02],\n",
      "        [1.3642e-02, 8.1470e-02, 3.3188e-02, 5.9533e-02, 7.9531e-02, 3.2936e-02,\n",
      "         5.2729e-02, 2.4819e-02, 2.2216e-02, 2.0574e-02],\n",
      "        [7.6493e-02, 5.3396e-02, 9.4844e-02, 4.5808e-02, 6.0277e-02, 5.1603e-02,\n",
      "         3.1919e-02, 7.0000e-02, 9.7986e-02, 2.7996e-02],\n",
      "        [9.8815e-02, 3.3516e-02, 1.9772e-02, 5.2546e-02, 1.6097e-02, 5.6565e-02,\n",
      "         2.5131e-02, 5.8420e-02, 5.0176e-02, 4.6645e-02],\n",
      "        [1.2568e-02, 6.7629e-03, 8.2640e-02, 1.0427e-02, 5.6533e-02, 1.0657e-02,\n",
      "         9.3121e-02, 4.0343e-02, 3.3685e-02, 8.3384e-02],\n",
      "        [7.8241e-02, 2.4308e-02, 6.6860e-02, 4.1536e-02, 6.8877e-02, 4.5780e-03,\n",
      "         2.3109e-02, 3.4744e-02, 6.8824e-02, 5.1140e-02],\n",
      "        [8.5166e-02, 2.3668e-02, 8.8247e-02, 1.5848e-02, 7.4486e-02, 7.2871e-02,\n",
      "         3.4516e-02, 5.1353e-02, 7.2375e-02, 4.8361e-02],\n",
      "        [5.4488e-02, 6.8569e-02, 5.6125e-02, 3.3850e-02, 5.7744e-02, 4.8498e-02,\n",
      "         3.5871e-02, 6.2937e-04, 9.2090e-02, 3.3738e-02],\n",
      "        [8.9135e-02, 9.3140e-02, 4.8845e-02, 6.7990e-02, 9.0707e-02, 9.7678e-02,\n",
      "         8.3787e-04, 4.0141e-02, 1.9889e-02, 2.0624e-02],\n",
      "        [4.6772e-02, 5.1053e-02, 8.2626e-02, 5.1579e-02, 5.3728e-02, 7.6766e-02,\n",
      "         8.2655e-02, 6.6029e-02, 8.8288e-02, 6.7226e-02],\n",
      "        [8.8944e-02, 3.7763e-02, 9.9763e-02, 9.8739e-02, 8.0075e-02, 3.9864e-02,\n",
      "         7.1470e-02, 6.4729e-02, 9.5548e-02, 2.5626e-02],\n",
      "        [8.1477e-02, 1.2588e-02, 1.9024e-02, 7.4729e-02, 3.6308e-02, 8.3360e-02,\n",
      "         1.8117e-02, 7.4765e-02, 5.9683e-02, 8.9707e-02],\n",
      "        [9.3800e-02, 2.4398e-02, 8.4314e-02, 4.8529e-03, 3.5399e-02, 6.6475e-02,\n",
      "         8.2151e-02, 1.7409e-03, 5.8754e-02, 9.1518e-02],\n",
      "        [5.4268e-02, 7.6115e-02, 5.9112e-02, 5.5984e-02, 8.5345e-03, 7.0317e-02,\n",
      "         9.7498e-03, 2.3969e-03, 2.7874e-02, 5.5194e-02],\n",
      "        [6.0489e-02, 1.9990e-02, 8.1815e-02, 3.5958e-02, 7.6176e-02, 8.3150e-02,\n",
      "         1.4025e-02, 6.1646e-02, 2.5856e-02, 2.0071e-02],\n",
      "        [5.8527e-02, 5.3794e-02, 1.0858e-02, 4.9768e-02, 4.2919e-02, 8.4429e-02,\n",
      "         5.2783e-02, 5.8505e-03, 3.2240e-02, 1.0395e-02],\n",
      "        [9.5374e-02, 3.5792e-02, 4.3373e-02, 5.1525e-02, 7.0240e-03, 4.5094e-02,\n",
      "         2.1807e-02, 6.3880e-02, 9.5552e-02, 5.1017e-02],\n",
      "        [7.1092e-03, 6.6892e-02, 5.0547e-02, 8.4910e-02, 6.1427e-02, 7.3472e-02,\n",
      "         3.6936e-02, 1.0573e-02, 2.0974e-02, 4.7501e-02],\n",
      "        [6.1171e-02, 8.6742e-02, 9.7644e-02, 4.3913e-02, 9.8885e-02, 4.8800e-02,\n",
      "         7.3249e-02, 7.3880e-02, 2.9437e-02, 6.9386e-02],\n",
      "        [5.0763e-02, 8.9320e-02, 9.6776e-02, 7.8453e-02, 2.0736e-02, 3.7100e-03,\n",
      "         6.5385e-03, 1.2322e-02, 3.0686e-02, 7.3849e-02],\n",
      "        [8.4784e-02, 5.6981e-02, 2.3986e-02, 6.7009e-02, 2.2603e-02, 8.7401e-02,\n",
      "         7.4795e-02, 4.3321e-02, 3.3698e-02, 8.8335e-02],\n",
      "        [3.4609e-02, 9.9478e-02, 3.9950e-02, 5.6495e-02, 1.9741e-02, 1.4853e-02,\n",
      "         3.4477e-02, 1.3600e-02, 2.6622e-02, 9.4546e-02],\n",
      "        [7.1617e-02, 6.1110e-02, 2.3923e-02, 8.5606e-02, 8.0742e-02, 4.6987e-02,\n",
      "         5.3998e-02, 8.8685e-02, 3.0398e-02, 4.3346e-02],\n",
      "        [5.7607e-02, 7.8854e-02, 7.0023e-02, 9.0177e-02, 1.1862e-02, 3.9505e-02,\n",
      "         7.9281e-02, 8.0466e-02, 2.3013e-02, 4.9528e-02],\n",
      "        [4.2091e-02, 7.9262e-04, 4.3464e-02, 3.2662e-02, 7.7406e-02, 7.3131e-02,\n",
      "         6.1166e-02, 7.9108e-02, 1.4277e-03, 4.4473e-02],\n",
      "        [1.9800e-02, 9.4599e-02, 3.0481e-02, 5.6923e-02, 1.4491e-02, 1.0116e-02,\n",
      "         6.7462e-02, 6.9139e-02, 6.6457e-02, 6.0681e-02],\n",
      "        [9.8186e-02, 8.6001e-02, 1.8089e-02, 2.0278e-02, 6.3701e-02, 7.0204e-03,\n",
      "         1.6363e-02, 9.5914e-02, 3.3490e-02, 1.2634e-02],\n",
      "        [2.9971e-02, 6.5188e-02, 6.1826e-02, 6.1635e-02, 9.9794e-02, 6.7857e-02,\n",
      "         6.0607e-02, 4.6178e-02, 4.1998e-02, 3.7184e-03],\n",
      "        [5.9563e-02, 7.2319e-02, 3.8700e-02, 5.6754e-02, 1.3702e-02, 4.3898e-02,\n",
      "         4.2666e-02, 6.8396e-02, 7.0961e-02, 8.7476e-02],\n",
      "        [6.6434e-02, 6.1212e-02, 4.8246e-02, 1.5571e-02, 4.7846e-02, 9.6698e-02,\n",
      "         2.5819e-02, 1.3260e-02, 5.1949e-02, 7.9898e-02],\n",
      "        [2.5417e-02, 2.4168e-02, 8.1022e-02, 9.7381e-02, 3.2443e-02, 2.1967e-02,\n",
      "         1.3231e-02, 5.2814e-02, 1.1841e-02, 7.2047e-02],\n",
      "        [4.3284e-02, 7.0409e-02, 6.2922e-02, 2.0142e-02, 8.3975e-02, 3.6765e-02,\n",
      "         4.0498e-02, 1.6440e-02, 8.3950e-02, 2.1873e-03],\n",
      "        [5.0977e-02, 8.1334e-02, 4.2606e-03, 4.4613e-02, 9.1312e-02, 7.9598e-02,\n",
      "         5.7179e-02, 5.3434e-02, 1.5954e-02, 8.7242e-02],\n",
      "        [1.3917e-02, 1.3696e-02, 8.7520e-02, 9.9558e-02, 4.8678e-02, 1.4147e-02,\n",
      "         1.3765e-02, 1.5118e-02, 2.1930e-02, 3.3146e-02]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "Original Matrix:  tensor([[0.2523, 0.2673, 0.2516,  ..., 0.2404, 0.2335, 0.2299],\n",
      "        [0.2510, 0.2557, 0.2522,  ..., 0.2408, 0.2545, 0.2319],\n",
      "        [0.2553, 0.2744, 0.2606,  ..., 0.2509, 0.2338, 0.2346],\n",
      "        ...,\n",
      "        [0.2757, 0.2669, 0.2629,  ..., 0.2485, 0.2663, 0.2576],\n",
      "        [0.2535, 0.2858, 0.2610,  ..., 0.2435, 0.2477, 0.2402],\n",
      "        [0.2511, 0.2792, 0.2687,  ..., 0.2504, 0.2464, 0.2281]],\n",
      "       dtype=torch.float64, grad_fn=<MmBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1000x2 and 784x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-bc6bec1a9c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtrain_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nLoss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#print(\"Final Matrix:\", torch.matmul(w,m))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-5184fb832eb0>\u001b[0m in \u001b[0;36mlinreg\u001b[0;34m(X, w, b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinreg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1000x2 and 784x100)"
     ]
    }
   ],
   "source": [
    "#First pair of weights\n",
    "www = np.random.rand(784,100) * 0.1\n",
    "w = torch.tensor(www, requires_grad=True)\n",
    "print(\"Weight Tensor #1: \", w)\n",
    "#w = torch.tensor(www, requires_grad=True) \n",
    "#First pair of biases\n",
    "b = torch.zeros(100, requires_grad=True)\n",
    "#printing out the weight and bias\n",
    "\n",
    "#Second pair of weights\n",
    "mm = np.random.rand(100,10) * 0.1\n",
    "m = torch.tensor(mm, requires_grad=True)\n",
    "print(\"Weight Tensor #2: \", m)\n",
    "#Second pair of bias\n",
    "b2 = torch.zeros(10, requires_grad=True)\n",
    "#printing out the second weight and bias\n",
    "\n",
    "#\n",
    "print(\"Original Matrix: \", torch.matmul(w,m))\n",
    "\n",
    "#learning rate\n",
    "lr = 0.05\n",
    "#Number of epoch\n",
    "num_epoch = 30\n",
    "#Performing linear regression\n",
    "net = linreg\n",
    "#Performing linear regression\n",
    "net2 = linreg\n",
    "#Performing sqaured loss\n",
    "loss = squared_loss\n",
    "#specified a number for the batch size\n",
    "batch_size = 30\n",
    "#using the weight decay within our sgd\n",
    "weight_decay = 1 - 1e-5\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for X,y in data_iter(batch_size, x_train.T, y_train.T):\n",
    "        l = cross_entropy(net2(net(X,w.float(),b.float()),m.float(),b2.float()),y)\n",
    "        l.sum().backward()\n",
    "        sgd([w,b], lr, batch_size, weight_decay)\n",
    "        sgd([m, b2], lr, batch_size, weight_decay)\n",
    "    with torch.no_grad():\n",
    "        train_l = cross_entropy(net2(net(x_test.T,w.float(),b.float()),m.float(),b2.float()),y_test.T)\n",
    "        print(\"Epoch:\", epoch+1, \"\\nLoss:\", float(train_l.mean()))\n",
    "#print(\"Final Matrix:\", torch.matmul(w,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
